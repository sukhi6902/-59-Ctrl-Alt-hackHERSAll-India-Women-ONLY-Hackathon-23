{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nfrom IPython.display import HTML\nfrom base64 import b64encode\nimport imgaug.augmenters as iaa\nimport math\n\nimport numpy as np\nimport cv2\nimport os\nfrom keras.models import load_model\nfrom collections import deque\nfrom tqdm import tqdm\nfrom sklearn.model_selection import StratifiedShuffleSplit\nimport matplotlib\nmatplotlib.use(\"Agg\")\nfrom keras.layers import Input\nfrom keras.models import Model\nfrom keras.layers.core import Dense\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.applications import ResNet50V2\nfrom keras import regularizers\nfrom keras.callbacks import Callback, ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping, ReduceLROnPlateau\nimport datetime\nimport seaborn as sns\nfrom sklearn import metrics\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-08-05T16:18:17.288450Z","iopub.execute_input":"2023-08-05T16:18:17.288918Z","iopub.status.idle":"2023-08-05T16:18:25.165547Z","shell.execute_reply.started":"2023-08-05T16:18:17.288882Z","shell.execute_reply":"2023-08-05T16:18:25.164455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# All the necessary packages have been imported.","metadata":{}},{"cell_type":"code","source":"PROJECT_DIR = '../input/real-life-violence-situations-dataset'","metadata":{"execution":{"iopub.status.busy":"2023-08-05T16:18:25.167712Z","iopub.execute_input":"2023-08-05T16:18:25.168305Z","iopub.status.idle":"2023-08-05T16:18:25.172738Z","shell.execute_reply.started":"2023-08-05T16:18:25.168275Z","shell.execute_reply":"2023-08-05T16:18:25.171735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!dir {PROJECT_DIR}","metadata":{"execution":{"iopub.status.busy":"2023-08-05T16:18:25.174207Z","iopub.execute_input":"2023-08-05T16:18:25.174885Z","iopub.status.idle":"2023-08-05T16:18:26.173053Z","shell.execute_reply.started":"2023-08-05T16:18:25.174846Z","shell.execute_reply":"2023-08-05T16:18:26.171695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import HTML\nfrom base64 import b64encode\n\ndef play(filename):\n    html = ''\n    video = open(filename,'rb').read()\n    src = 'data:video/mp4;base64,' + b64encode(video).decode()\n    html += '<video width=640 muted controls autoplay loop><source src=\"%s\" type=\"video/mp4\"></video>' % src \n    return HTML(html)","metadata":{"execution":{"iopub.status.busy":"2023-08-05T16:18:26.176560Z","iopub.execute_input":"2023-08-05T16:18:26.177528Z","iopub.status.idle":"2023-08-05T16:18:26.184080Z","shell.execute_reply.started":"2023-08-05T16:18:26.177485Z","shell.execute_reply":"2023-08-05T16:18:26.182999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Function to play the video for violence detection.","metadata":{}},{"cell_type":"code","source":"play('../input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence/V_192.mp4')","metadata":{"execution":{"iopub.status.busy":"2023-08-05T16:18:26.185731Z","iopub.execute_input":"2023-08-05T16:18:26.186403Z","iopub.status.idle":"2023-08-05T16:18:26.218106Z","shell.execute_reply.started":"2023-08-05T16:18:26.186364Z","shell.execute_reply":"2023-08-05T16:18:26.216940Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMG_SIZE = 128\nColorChannels = 3\n\ndef video_to_frames(video):\n    vidcap = cv2.VideoCapture(video)\n    \n    \n    rate = math.floor(vidcap.get(3))\n    count = 0\n    \n    ImageFrames = []\n    while vidcap.isOpened():\n        ID = vidcap.get(1)\n        success, image = vidcap.read()\n        \n        if success:\n            # skipping frames to avoid duplications \n            if (ID % 7 == 0):\n                flip = iaa.Fliplr(1.0)\n                zoom = iaa.Affine(scale=1.3)\n                random_brightness = iaa.Multiply((1, 1.3))\n                rotate = iaa.Affine(rotate=(-25, 25))\n                \n                image_aug = flip(image = image)\n                image_aug = random_brightness(image = image_aug)\n                image_aug = zoom(image = image_aug)\n                image_aug = rotate(image = image_aug)\n                \n                rgb_img = cv2.cvtColor(image_aug, cv2.COLOR_BGR2RGB)\n                resized = cv2.resize(rgb_img, (IMG_SIZE, IMG_SIZE))\n                ImageFrames.append(resized)\n                \n            count += 1\n        else:\n            break\n    \n    vidcap.release()\n    \n    return ImageFrames","metadata":{"execution":{"iopub.status.busy":"2023-08-05T16:18:26.219195Z","iopub.execute_input":"2023-08-05T16:18:26.219870Z","iopub.status.idle":"2023-08-05T16:18:26.233441Z","shell.execute_reply.started":"2023-08-05T16:18:26.219813Z","shell.execute_reply":"2023-08-05T16:18:26.232433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Function to split the video into frames to analyse.","metadata":{}},{"cell_type":"code","source":"%%time\nfrom tqdm import tqdm\n\nVideoDataDir = PROJECT_DIR + '/Real Life Violence Dataset'\nprint('we have \\n{} Violence videos \\n{} NonViolence videos'.format(\n              len(os.listdir(VideoDataDir + '/Violence')), \n              len(os.listdir(VideoDataDir + '/NonViolence'))))\n\nX_original = []\ny_original = []\n\nprint('Choosing 200 videos out of 1000 for each, cuz of memory issue')\nCLASSES = [\"NonViolence\", \"Violence\"]\n\n\nfor category in os.listdir(VideoDataDir):\n    path = os.path.join(VideoDataDir, category)\n    class_num = CLASSES.index(category)\n    for i, video in enumerate(tqdm(os.listdir(path)[0:200])):\n        frames = video_to_frames(path + '/' + video)\n        for j, frame in enumerate(frames):\n            X_original.append(frame)\n            y_original.append(class_num)","metadata":{"execution":{"iopub.status.busy":"2023-08-05T16:18:26.236806Z","iopub.execute_input":"2023-08-05T16:18:26.237626Z","iopub.status.idle":"2023-08-05T16:20:41.351909Z","shell.execute_reply.started":"2023-08-05T16:18:26.237588Z","shell.execute_reply":"2023-08-05T16:20:41.349470Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating Violence and Non Violence Dataset and using 200 images due to RAM constraints.","metadata":{}},{"cell_type":"code","source":"X_original = np.array(X_original).reshape(-1 , IMG_SIZE * IMG_SIZE * 3)\ny_original = np.array(y_original)\nlen(X_original)","metadata":{"execution":{"iopub.status.busy":"2023-08-05T16:20:41.353430Z","iopub.execute_input":"2023-08-05T16:20:41.354042Z","iopub.status.idle":"2023-08-05T16:20:41.486471Z","shell.execute_reply.started":"2023-08-05T16:20:41.353999Z","shell.execute_reply":"2023-08-05T16:20:41.485281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stratified_sample = StratifiedShuffleSplit(n_splits=2, test_size=0.3, random_state=73)\n\nfor train_index, test_index in stratified_sample.split(X_original, y_original):\n    X_train, X_test = X_original[train_index], X_original[test_index]\n    y_train, y_test = y_original[train_index], y_original[test_index]\n\nX_train_nn = X_train.reshape(-1, IMG_SIZE, IMG_SIZE, 3) / 255\nX_test_nn = X_test.reshape(-1, IMG_SIZE, IMG_SIZE, 3) / 255","metadata":{"execution":{"iopub.status.busy":"2023-08-05T16:20:41.488027Z","iopub.execute_input":"2023-08-05T16:20:41.488516Z","iopub.status.idle":"2023-08-05T16:20:42.843620Z","shell.execute_reply.started":"2023-08-05T16:20:41.488476Z","shell.execute_reply":"2023-08-05T16:20:42.842306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reshaping the image and using Stratified Shuffle Split.","metadata":{}},{"cell_type":"code","source":"epochs = 150\n\n\nkernel_regularizer = regularizers.l2(0.001)\n\n\ndef load_layers():\n    input_tensor = Input(shape=(IMG_SIZE, IMG_SIZE, ColorChannels))\n    baseModel = ResNet50V2(weights = \"imagenet\", pooling='avg',\n                            include_top=False, \n                            input_tensor=input_tensor)\n    \n    headModel = baseModel.output   \n    headModel = Dense(1, activation=\"sigmoid\")(headModel)\n    model = Model(inputs=baseModel.input, outputs=headModel)\n\n    for layer in baseModel.layers:\n        layer.trainable = False\n\n    print(\"Compiling model...\")\n    model.compile(loss=\"binary_crossentropy\",\n                    optimizer='adam',\n                    metrics=[\"accuracy\"])\n\n    return model\n\n\nmodel = load_layers()\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-08-05T16:20:42.847219Z","iopub.execute_input":"2023-08-05T16:20:42.851382Z","iopub.status.idle":"2023-08-05T16:20:48.626051Z","shell.execute_reply.started":"2023-08-05T16:20:42.851337Z","shell.execute_reply":"2023-08-05T16:20:48.624963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training the model for 150 epochs using ResNet50V2 and using imagenet weights.","metadata":{}},{"cell_type":"code","source":"\n\npatience = 3\n\nstart_lr = 0.00001\nmin_lr = 0.00001\nmax_lr = 0.00005\n\nbatch_size = 20\n\n\n\nrampup_epochs = 30\nsustain_epochs = 0\nexp_decay = .8\n\ndef lrfn(epoch):\n    if epoch < rampup_epochs:\n        return (max_lr - start_lr)/rampup_epochs * epoch + start_lr\n    elif epoch < rampup_epochs + sustain_epochs:\n        return max_lr\n    else:\n        return (max_lr - min_lr) * exp_decay**(epoch-rampup_epochs-sustain_epochs) + min_lr\n\n\nclass myCallback(Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if ((logs.get('accuracy')>=0.99)):\n            print(\"\\nLimits Reached cancelling training!\")\n            self.model.stop_training = True","metadata":{"execution":{"iopub.status.busy":"2023-08-05T16:22:15.483268Z","iopub.execute_input":"2023-08-05T16:22:15.483898Z","iopub.status.idle":"2023-08-05T16:22:15.499720Z","shell.execute_reply.started":"2023-08-05T16:22:15.483848Z","shell.execute_reply":"2023-08-05T16:22:15.498481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Stopping early if the accuracy is >= 99%.","metadata":{}},{"cell_type":"code","source":"end_callback = myCallback()\n\nlr_callback = LearningRateScheduler(lambda epoch: lrfn(epoch), verbose=False)\n\nearly_stopping = EarlyStopping(patience = patience, monitor='val_loss',\n                                 mode='min', restore_best_weights=True, \n                                 verbose = 1, min_delta = .00075)\n\n\n\nlr_plat = ReduceLROnPlateau(patience = 2, mode = 'min')\n\nos.system('rm -rf ./logs/')\n\n\nlog_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\ntensorboard_callback = TensorBoard(log_dir = log_dir, write_graph=True, histogram_freq=1)\n\ncheckpoint_filepath = 'ModelWeights.h5'\n\nmodel_checkpoints = ModelCheckpoint(filepath=checkpoint_filepath,\n                                        save_weights_only=True,\n                                        monitor='val_loss',\n                                        mode='min',\n                                        verbose = 1,\n                                        save_best_only=True)\n\n\ncallbacks = [end_callback, lr_callback, model_checkpoints, tensorboard_callback, early_stopping, lr_plat]\n\n","metadata":{"execution":{"iopub.status.busy":"2023-08-05T16:22:15.994792Z","iopub.execute_input":"2023-08-05T16:22:15.995647Z","iopub.status.idle":"2023-08-05T16:22:16.691828Z","shell.execute_reply.started":"2023-08-05T16:22:15.995603Z","shell.execute_reply":"2023-08-05T16:22:16.690752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Callback function to save weights if the loss is reduced.","metadata":{}},{"cell_type":"code","source":"print('Training head...')\nbatch_size = 50\n#model.load_weights('./Model_Weights.h5')\n\nhistory = model.fit(X_train_nn ,y_train, epochs=epochs,\n                        callbacks=callbacks,\n                        validation_data = (X_test_nn, y_test),\n                        batch_size=batch_size)\n\nprint('\\nRestoring best Weights for ResNet50V2')\nmodel.load_weights(checkpoint_filepath)","metadata":{"execution":{"iopub.status.busy":"2023-08-05T16:22:16.693760Z","iopub.execute_input":"2023-08-05T16:22:16.694647Z","iopub.status.idle":"2023-08-05T16:54:16.015255Z","shell.execute_reply.started":"2023-08-05T16:22:16.694606Z","shell.execute_reply":"2023-08-05T16:54:16.011256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training the model.","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\n\ndef print_graph(item, index, history):\n    plt.figure()\n    train_values = history.history[item][0:index]\n    plt.plot(train_values)\n    test_values = history.history['val_' + item][0:index]\n    plt.plot(test_values)\n    plt.legend(['training','validation'])\n    plt.title('Training and validation '+ item)\n    plt.xlabel('epoch')\n    plt.show()\n    plot = '{}.png'.format(item)\n    plt.savefig(plot)\n\n\ndef get_best_epoch(test_loss, history):\n    for key, item in enumerate(history.history.items()):\n        (name, arr) = item\n        if name == 'val_loss':\n            for i in range(len(arr)):\n                if round(test_loss, 2) == round(arr[i], 2):\n                    return i\n                \ndef model_summary(model, history):\n    print('---'*30)\n    test_loss, test_accuracy = model.evaluate(X_test_nn, y_test, verbose=0)\n\n    if history:\n        index = get_best_epoch(test_loss, history)\n        print('Best Epochs: ', index)\n\n        train_accuracy = history.history['accuracy'][index]\n        train_loss = history.history['loss'][index]\n\n        print('Accuracy on train:',train_accuracy,'\\tLoss on train:',train_loss)\n        print('Accuracy on test:',test_accuracy,'\\tLoss on test:',test_loss)\n        print_graph('loss', index, history)\n        print_graph('accuracy', index, history)\n        print('---'*30)","metadata":{"execution":{"iopub.status.busy":"2023-08-05T16:54:16.017094Z","iopub.execute_input":"2023-08-05T16:54:16.018333Z","iopub.status.idle":"2023-08-05T16:54:16.040733Z","shell.execute_reply.started":"2023-08-05T16:54:16.018282Z","shell.execute_reply":"2023-08-05T16:54:16.039726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_summary(model, history)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-05T16:54:16.051119Z","iopub.execute_input":"2023-08-05T16:54:16.052035Z","iopub.status.idle":"2023-08-05T16:54:21.638345Z","shell.execute_reply.started":"2023-08-05T16:54:16.051994Z","shell.execute_reply":"2023-08-05T16:54:21.637275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training and validation accuracy is increasing while the loss is decreasing.","metadata":{}},{"cell_type":"code","source":"print(\"Evaluating network...\")\npredictions = model.predict(X_test_nn)\npreds = predictions > 0.6","metadata":{"execution":{"iopub.status.busy":"2023-08-05T16:54:21.650497Z","iopub.execute_input":"2023-08-05T16:54:21.651156Z","iopub.status.idle":"2023-08-05T16:54:26.006568Z","shell.execute_reply.started":"2023-08-05T16:54:21.651114Z","shell.execute_reply":"2023-08-05T16:54:26.005499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Only choosing those images where the prediction accuracy is more than 60% sure whether the images are violence or non violence. ","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, roc_auc_score, plot_roc_curve, accuracy_score, classification_report, confusion_matrix\ncorr_pred = metrics.confusion_matrix(y_test, preds)\n\ncorr_pred = metrics.confusion_matrix(y_test, preds)\n\nn_correct = np.int((corr_pred[0][0] + corr_pred[1][1]))\nprint('> Correct Predictions:', n_correct)\nn_wrongs = np.int((corr_pred[0][1] + (corr_pred[1][0])))\nprint('> Wrong Predictions:', n_wrongs)\n\nsns.heatmap(corr_pred,annot=True, fmt=\"d\",cmap=\"Blues\")\nplt.show()\n\nprint(metrics.classification_report(y_test, preds, \n                           target_names=[\"NonViolence\", \"Violence\"]))","metadata":{"execution":{"iopub.status.busy":"2023-08-05T16:54:26.014805Z","iopub.execute_input":"2023-08-05T16:54:26.015124Z","iopub.status.idle":"2023-08-05T16:54:26.269736Z","shell.execute_reply.started":"2023-08-05T16:54:26.015094Z","shell.execute_reply":"2023-08-05T16:54:26.268606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Confusion matrix to show the accuracy.","metadata":{}},{"cell_type":"code","source":"args_model = \"model.h5\"\nmodel.save(args_model)","metadata":{"execution":{"iopub.status.busy":"2023-08-05T16:54:26.271464Z","iopub.execute_input":"2023-08-05T16:54:26.271862Z","iopub.status.idle":"2023-08-05T16:54:26.716923Z","shell.execute_reply.started":"2023-08-05T16:54:26.271822Z","shell.execute_reply":"2023-08-05T16:54:26.715859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport argparse\nimport pickle\nimport cv2\nimport os\nimport time\nfrom keras.models import load_model\nfrom collections import deque","metadata":{"execution":{"iopub.status.busy":"2023-08-05T16:54:26.718682Z","iopub.execute_input":"2023-08-05T16:54:26.719092Z","iopub.status.idle":"2023-08-05T16:54:26.726179Z","shell.execute_reply.started":"2023-08-05T16:54:26.719051Z","shell.execute_reply":"2023-08-05T16:54:26.725023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ndef print_results(video, limit=None):\n        fig=plt.figure(figsize=(16, 30))\n        if not os.path.exists('output'):\n            os.mkdir('output')\n\n        print(\"Loading model ...\")\n        model = load_model('./model.h5')\n        Q = deque(maxlen=128)\n\n        vs = cv2.VideoCapture(video)\n        writer = None\n        (W, H) = (None, None)\n        count = 0     \n        while True:\n                (grabbed, frame) = vs.read()\n                ID = vs.get(1)\n                if not grabbed:\n                    break\n                try:\n                    if (ID % 7 == 0):\n                        count = count + 1\n                        n_frames = len(frame)\n                        \n                        if W is None or H is None:\n                            (H, W) = frame.shape[:2]\n\n                        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n                        output = cv2.resize(frame, (512, 360)).copy()\n                        frame = cv2.resize(frame, (128, 128)).astype(\"float16\")\n                        frame = frame.reshape(IMG_SIZE, IMG_SIZE, 3) / 255\n                        preds = model.predict(np.expand_dims(frame, axis=0))[0]\n                        Q.append(preds)\n\n                        results = np.array(Q).mean(axis=0)\n                        i = (preds > 0.6)[0] #np.argmax(results)\n\n                        label = i\n\n                        text = \"Violence: {}\".format(label)\n                        #print('prediction:', text)\n                        file = open(\"output.txt\",'w')\n                        file.write(text)\n                        file.close()\n\n                        color = (0, 255, 0)\n\n                        if label:\n                            color = (255, 0, 0) \n                        else:\n                            color = (0, 255, 0)\n\n                        cv2.putText(output, text, (35, 50), cv2.FONT_HERSHEY_SIMPLEX,\n                                1, color, 3)\n\n\n                        # saving mp4 with labels but cv2.imshow is not working with this notebook\n                        if writer is None:\n                                fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n                                writer = cv2.VideoWriter(\"output.mp4\", fourcc, 60,\n                                        (W, H), True)\n\n                        writer.write(output)\n                        #cv2.imshow(\"Output\", output)\n\n                        fig.add_subplot(8, 3, count)\n                        plt.imshow(output)\n\n                    if limit and count > limit:\n                        break\n\n                except:\n                    break \n        \n        plt.show()\n        print(\"Cleaning up...\")\n        if writer is not None:\n            writer.release()\n        vs.release()","metadata":{"execution":{"iopub.status.busy":"2023-08-05T16:54:26.727791Z","iopub.execute_input":"2023-08-05T16:54:26.728458Z","iopub.status.idle":"2023-08-05T16:54:26.748509Z","shell.execute_reply.started":"2023-08-05T16:54:26.728420Z","shell.execute_reply":"2023-08-05T16:54:26.747506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Violence=r\"../input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence/V_1.mp4\" \nplay(Violence)","metadata":{"execution":{"iopub.status.busy":"2023-08-05T18:34:24.906164Z","iopub.execute_input":"2023-08-05T18:34:24.906599Z","iopub.status.idle":"2023-08-05T18:34:25.037508Z","shell.execute_reply.started":"2023-08-05T18:34:24.906512Z","shell.execute_reply":"2023-08-05T18:34:25.036160Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/2524064070.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mViolence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mr\"../input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence/V_1.mp4\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mViolence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'play' is not defined"],"ename":"NameError","evalue":"name 'play' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"# The model is able to classify the frames with 90% accuracy.","metadata":{}},{"cell_type":"code","source":"print_results(Violence, limit=30)","metadata":{"execution":{"iopub.status.busy":"2023-08-05T16:54:26.937550Z","iopub.execute_input":"2023-08-05T16:54:26.938404Z","iopub.status.idle":"2023-08-05T16:54:34.093077Z","shell.execute_reply.started":"2023-08-05T16:54:26.938354Z","shell.execute_reply":"2023-08-05T16:54:34.091701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NonViolence=r\"../input/real-life-violence-situations-dataset/Real Life Violence Dataset/NonViolence/NV_10.mp4\"\nplay(NonViolence)","metadata":{"execution":{"iopub.status.busy":"2023-08-05T16:54:34.094756Z","iopub.execute_input":"2023-08-05T16:54:34.095412Z","iopub.status.idle":"2023-08-05T16:54:34.160005Z","shell.execute_reply.started":"2023-08-05T16:54:34.095373Z","shell.execute_reply":"2023-08-05T16:54:34.159039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print_results(NonViolence, limit=30)","metadata":{"execution":{"iopub.status.busy":"2023-08-05T16:54:34.161357Z","iopub.execute_input":"2023-08-05T16:54:34.162340Z","iopub.status.idle":"2023-08-05T16:54:40.691594Z","shell.execute_reply.started":"2023-08-05T16:54:34.162300Z","shell.execute_reply":"2023-08-05T16:54:40.690437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Thus, we have obtained a Violence Detection Surveillance System with high accuracy and very little loss. We use ResNetV2 as it has many attributes which will give the highest accuracy for a model.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}